{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8d613fb",
   "metadata": {},
   "source": [
    "## Tutorial: Getting Started"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe083229",
   "metadata": {},
   "source": [
    "### 0Ô∏è‚É£ Set up environment and install dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344688d6",
   "metadata": {},
   "source": [
    "Please make sure you have set up the environment and installed required libraries by following the steps in the rl-swarm README.md and running the `run_rl_swarm.sh` script"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc782b7",
   "metadata": {},
   "source": [
    "### 1Ô∏è‚É£ Import dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6c3e33",
   "metadata": {},
   "source": [
    "In this step, we import all the necessary libraries and set up logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a74bc8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Get the path to src/ relative to current notebook\n",
    "src_path = os.path.join(os.getcwd(), 'src')\n",
    "if src_path not in sys.path:\n",
    "    sys.path.insert(0, src_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5775154d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shikharrastogi/genrl-swarm-zh1p4ng-private/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from typing import List\n",
    "\n",
    "# Import huggingface transformers for loading pre-trained language models\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "# Import genrl_swarm modules for data, game management, rewards, and training\n",
    "from genrl_swarm.communication.distributed.null_comm import NullCommunicationBackend\n",
    "from genrl_swarm.data.text_data_managers import SimpleTextDataManager\n",
    "from genrl_swarm.game import BaseGameManager\n",
    "from genrl_swarm.state import GameState\n",
    "from genrl_swarm.rewards import text_games_reward_utils\n",
    "from genrl_swarm.trainer.GRPOTrainer import GRPOTrainerModule\n",
    "\n",
    "import logging\n",
    "\n",
    "# Set up root logger to display INFO level logs\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "\n",
    "# This ensures that log messages from imported modules propagate to the root logger\n",
    "logging.getLogger().setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3ee75c",
   "metadata": {},
   "source": [
    "### 2Ô∏è‚É£ Define Constants and Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a665b176",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# We define maximum number of rounds of RL training. Each round can consist of multiple stages but we limit this example to a single stage.\n",
    "MAX_ROUNDS = 10\n",
    "MAX_STAGES = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b5f2f32",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# The GSM8k dataset stores answers like \"#### 42\". We define a function to extract that answer.\n",
    "# You can modify this function if you're working with a different dataset format.\n",
    "def extract_hash_answer(text: str) -> str | None: \n",
    "    if \"####\" not in text:\n",
    "        return None\n",
    "    return text.split(\"####\")[1].strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa6c34a",
   "metadata": {},
   "source": [
    "### 3Ô∏è‚É£ Prepare the Dataset with SimpleTextDataManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a130a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The system prompt guides the model to produce answers in a specific format.\n",
    "# You can modify this to control how the model should think and answer.\n",
    "# This system prompt will be prepended to each question for both training and evaluation.\n",
    "SYSTEM_PROMPT = \"\"\"\n",
    "You are given a math problem, and you want to come up with the best possible answer. \n",
    "Think through the solution of the problem step by step and then state your final answer.\n",
    "An ideal solution will satisfy three important criteria:\n",
    "  1) Correct step-by-step reasoning.\n",
    "  2) Clear and concise explanation.\n",
    "  3) Final answer in the form: Answer: $Answer (without quotes)\n",
    "Remember to put your answer on its own line after \\\"Answer:\\\".\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3347edba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SimpleTextDataManager handles dataset loading, preprocessing and feeding into the RL game.\n",
    "# You can modify num_train_samples or num_evaluation_samples for larger or smaller training sets.\n",
    "data_manager = SimpleTextDataManager(\n",
    "    train_dataset=\"openai/gsm8k\",\n",
    "    evaluation_dataset=\"openai/gsm8k\",\n",
    "    data_subset=\"main\",\n",
    "    num_train_samples=2,\n",
    "    column_name_map={'question': 'question', 'answer': 'answer'},\n",
    "    column_preprocessing_map={'answer': extract_hash_answer},\n",
    "    system_prompt=SYSTEM_PROMPT\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ff794c",
   "metadata": {},
   "source": [
    "### 4Ô∏è‚É£ Define Reward Conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9d731289",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we define two types of reward conditions:\n",
    "# - format_reward_condition checks if the model produces output in correct format\n",
    "# - correctness_reward_condition checks if the actual answer matches expected answer\n",
    "# You can adjust weights to control the importance of format vs correctness.\n",
    "reward_conditions = [\n",
    "    text_games_reward_utils.format_reward_condition(pattern=r\"\\nAnswer: \\d+\", weight=0.5),\n",
    "    text_games_reward_utils.correctness_reward_condition(\n",
    "        pattern=r'Answer: .*?([\\d,]+(?:\\.\\d+)?)', weight=2.0)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f130e84",
   "metadata": {},
   "source": [
    "### 5Ô∏è‚É£ Test Reward Function (Sanity Check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "39fb5d03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.5, 0.5, 2.5, 0.0]\n"
     ]
    }
   ],
   "source": [
    "# Before running full training, it's useful to manually verify reward calculation.\n",
    "# We create some example completions and check how rewards are assigned.\n",
    "completions = [\n",
    "    \"Question: 2+2\\nAnswer: 4\", \n",
    "    \"Question: 2+2\\nAnswer: 5\",\n",
    "    \"Question: 2+2\\nAnswer: 4.000\",\n",
    "    \"Question: 2+1\\nAnswer 3\"\n",
    "]\n",
    "\n",
    "correct_answers = [4, 4, 4, 3]\n",
    "\n",
    "# Calculate rewards for these samples\n",
    "rewards = text_games_reward_utils.calculate_reward(\n",
    "    completions=completions,\n",
    "    correct_answers=correct_answers,\n",
    "    reward_conditions=reward_conditions\n",
    ")\n",
    "\n",
    "print(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7d4447aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RewardManager manages how rewards are computed during training rounds. We initialize reward manager here with the reward conditions we defined in the previous step.\n",
    "reward_manager = text_games_reward_utils.get_default_reward_manager(\n",
    "    reward_conditions=reward_conditions, \n",
    "    max_rounds=MAX_ROUNDS\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "900e1cdc",
   "metadata": {},
   "source": [
    "### 6Ô∏è‚É£ Load Model and Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "00ca7ec2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    }
   ],
   "source": [
    "# We load a pretrained language model from HuggingFace.\n",
    "# You can swap this model to experiment with different LLM backbones.\n",
    "model_name = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "models = [AutoModelForCausalLM.from_pretrained(model_name)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "610cd075",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-23 15:02:42,811 - genrl_swarm.logging_utils.global_defs - INFO - Invalid log type: None. Default to terminal logging\n"
     ]
    }
   ],
   "source": [
    "# GRPOTrainerModule handles reinforcement learning updates.\n",
    "# This is where the RL optimization happens.\n",
    "trainer = GRPOTrainerModule(models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7733beb5",
   "metadata": {},
   "source": [
    "### 7Ô∏è‚É£ Initialize the Game Manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "153c8e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GameState keeps track of the current round and stage.\n",
    "game_state = GameState(round=0, stage=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e4370e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BaseGameManager orchestrates the full RL game loop.\n",
    "# You can adjust max_stage and max_round for more complex multi-stage setups.\n",
    "game_manager = BaseGameManager(\n",
    "    max_stage=MAX_STAGES,\n",
    "    max_round=MAX_ROUNDS,\n",
    "    game_state=game_state,\n",
    "    reward_manager=reward_manager,\n",
    "    trainer=trainer,\n",
    "    data_manager=data_manager,\n",
    "    run_mode=\"train\",\n",
    "    communication=NullCommunicationBackend()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "52f5e5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining an evaluation function to calculate rewards for the evaluation data, game_state, rewards and trainer defined by game_manager.\n",
    "@torch.no_grad()\n",
    "def evaluate(game_manager: BaseGameManager) -> List[float]:\n",
    "    completions = []\n",
    "    correct_answers = []\n",
    "\n",
    "    eval_data = game_manager.data_manager.get_eval_data(split='test')[:10]\n",
    "    for idx, world_state in eval_data:\n",
    "        prompt = [\n",
    "                {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "                {\"role\": \"user\", \"content\": world_state.environment_states['question']}\n",
    "            ]\n",
    "        input_ids = game_manager.trainer.processing_class.apply_chat_template(prompt, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\")\n",
    "        input_ids = input_ids.to(game_manager.trainer.model.device)\n",
    "        outputs = game_manager.trainer.model(input_ids)\n",
    "        outputs = game_manager.trainer.model.generate(input_ids, attention_mask = torch.ones_like(input_ids), generation_config=game_manager.trainer.generation_config)\n",
    "\n",
    "        answer = game_manager.trainer.processing_class.decode(outputs[0], skip_special_tokens=True)\n",
    "        completions.append(answer)\n",
    "        correct_answers.append(answer)\n",
    "    \n",
    "    rewards = text_games_reward_utils.calculate_reward(\n",
    "        completions=completions,\n",
    "        correct_answers=correct_answers,\n",
    "        reward_conditions=reward_conditions\n",
    "    )\n",
    "\n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "40653980",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Average reward for model before GRPO training: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Let's evaluate the model before we start training\n",
    "untrained_model_rewards = evaluate(game_manager)\n",
    "print(untrained_model_rewards)\n",
    "print(\"Average reward for model before GRPO training:\", sum(untrained_model_rewards) / len(untrained_model_rewards))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ee55c0",
   "metadata": {},
   "source": [
    "### 8Ô∏è‚É£ Run the Game Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "01754b17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-23 15:03:50,651 - genrl_swarm.logging_utils.global_defs - INFO - Starting round: 1/10.\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00, 595.78 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00, 1146.61 examples/s]\n",
      "2025-06-23 15:04:17,614 - genrl_swarm.logging_utils.global_defs - INFO - {'train/loss': 0.13290411233901978, 'train/rewards': 0.5}\n",
      "2025-06-23 15:04:19,293 - genrl_swarm.logging_utils.global_defs - INFO - Starting round: 2/10.\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00, 728.11 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00, 1209.95 examples/s]\n",
      "2025-06-23 15:04:27,014 - genrl_swarm.logging_utils.global_defs - INFO - {'train/loss': 0.0, 'train/rewards': 0.0}\n",
      "2025-06-23 15:04:29,247 - genrl_swarm.logging_utils.global_defs - INFO - Starting round: 3/10.\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00, 486.04 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00, 1450.06 examples/s]\n",
      "2025-06-23 15:04:44,409 - genrl_swarm.logging_utils.global_defs - INFO - {'train/loss': 0.14718927443027496, 'train/rewards': 1.5}\n",
      "2025-06-23 15:04:46,462 - genrl_swarm.logging_utils.global_defs - INFO - Starting round: 4/10.\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00, 567.87 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00, 1250.54 examples/s]\n",
      "2025-06-23 15:04:57,907 - genrl_swarm.logging_utils.global_defs - INFO - {'train/loss': 0.4280991852283478, 'train/rewards': 0.5}\n",
      "2025-06-23 15:04:59,666 - genrl_swarm.logging_utils.global_defs - INFO - Starting round: 5/10.\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00, 1220.87 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00, 1235.25 examples/s]\n",
      "2025-06-23 15:05:11,389 - genrl_swarm.logging_utils.global_defs - INFO - {'train/loss': -0.06418755650520325, 'train/rewards': 0.5}\n",
      "2025-06-23 15:05:13,177 - genrl_swarm.logging_utils.global_defs - INFO - Starting round: 6/10.\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00, 862.85 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00, 1354.75 examples/s]\n",
      "2025-06-23 15:05:18,574 - genrl_swarm.logging_utils.global_defs - INFO - {'train/loss': -0.07790159434080124, 'train/rewards': 0.625}\n",
      "2025-06-23 15:05:20,368 - genrl_swarm.logging_utils.global_defs - INFO - Starting round: 7/10.\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00, 594.39 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00, 1380.39 examples/s]\n",
      "2025-06-23 15:05:37,430 - genrl_swarm.logging_utils.global_defs - INFO - {'train/loss': 0.1250694990158081, 'train/rewards': 0.5}\n",
      "2025-06-23 15:05:38,882 - genrl_swarm.logging_utils.global_defs - INFO - Starting round: 8/10.\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00, 1017.29 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00, 1282.86 examples/s]\n",
      "2025-06-23 15:05:54,687 - genrl_swarm.logging_utils.global_defs - INFO - {'train/loss': -0.06232127919793129, 'train/rewards': 0.625}\n",
      "2025-06-23 15:05:56,457 - genrl_swarm.logging_utils.global_defs - INFO - Starting round: 9/10.\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00, 712.29 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00, 1239.82 examples/s]\n",
      "2025-06-23 15:06:05,054 - genrl_swarm.logging_utils.global_defs - INFO - {'train/loss': -0.014482916332781315, 'train/rewards': 0.5}\n",
      "2025-06-23 15:06:06,597 - genrl_swarm.logging_utils.global_defs - INFO - Starting round: 10/10.\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00, 520.29 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00, 1019.40 examples/s]\n",
      "2025-06-23 15:06:08,783 - genrl_swarm.logging_utils.global_defs - INFO - {'train/loss': 0.0, 'train/rewards': 0.0}\n"
     ]
    }
   ],
   "source": [
    "# This kicks off the reinforcement learning game!\n",
    "# It will repeatedly generate completions, and begin training the model.\n",
    "game_manager.run_game()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f212c1a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.5, 0.0, 0.5, 0.0, 0.0, 0.5, 0.5, 0.0, 0.0]\n",
      "Average reward for model after GRPO training: 0.2\n"
     ]
    }
   ],
   "source": [
    "new_rewards = evaluate(game_manager)\n",
    "print(new_rewards)\n",
    "print(\"Average reward for model after GRPO training:\", sum(new_rewards) / len(new_rewards))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d59c115",
   "metadata": {},
   "source": [
    "### ‚úÖ Summary\n",
    "- Loaded GSM8k dataset with preprocessing\n",
    "- Defined reward functions\n",
    "- Set up the genrl_swarm framework for RL\n",
    "- Ran single-agent single-stage RL training using GRPO\n",
    "The full genrl_swarm package allows much more complex multi-agent setups."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f25a4c7a",
   "metadata": {},
   "source": [
    "### üëâ Next Steps\n",
    "\n",
    "Looking to build your own swarms? Start by looking at examples in the `genrl_swarm/examples` folder. They can be run by executing the corresponding recipe configuration in the `genrl_swarm/recipes` folder by changing the final line in the launch script `run_rl_swarm.sh` to point to the new configuration.\n",
    "\n",
    "For example, to run the multistage version of gsm8k, you can run the following command while in the root `rl-swarm-private` repository. Make sure to set the relevant environment variables like done in the run_rl_swarm script.\n",
    "\n",
    "```bash  \n",
    "python \"$ROOT/genrl-swarm-zh1p4ng/src/genrl_swarm/runner/swarm_launcher.py\" \\\n",
    "    --config-path \"$ROOT/genrl-swarm-zh1p4ng/recipes/multistage_math\" \\\n",
    "    --config-name \"msm_gsm8k_grpo.yaml\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "396a9cbe",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
