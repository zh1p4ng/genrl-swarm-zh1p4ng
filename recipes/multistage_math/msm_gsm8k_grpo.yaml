log_dir: ${oc.env:ROOT,.}/logs 

training:
  max_round: 2
  max_stage: 3

game_manager:
  _target_: genrl_swarm.game.game_manager.BaseGameManager
  max_stage: ${training.max_stage}
  max_round: ${training.max_round}
  game_state: 
    _target_: genrl_swarm.state.GameState
    round: 0
    stage: 0
  reward_manager:
    _target_: genrl_swarm.rewards.DefaultRewardManager
    reward_fn_store:
      _target_: genrl_swarm.rewards.reward_store.RewardFnStore
      max_rounds: ${training.max_round}
      reward_fn_stores:
        - _target_: genrl_swarm.rewards.reward_store.RoundRewardFnStore
          num_stages: ${training.max_stage}
          reward_fns:
            - _target_: genrl_swarm.examples.multistage_math.rewards.Stage0Rewards
            - _target_: genrl_swarm.examples.multistage_math.rewards.Stage1Rewards
            - _target_: genrl_swarm.examples.multistage_math.rewards.Stage2Rewards
  trainer:
    _target_: genrl_swarm.examples.multistage_math.trainer.GRPOTrainerModule
    models:
      - _target_: transformers.AutoModelForCausalLM.from_pretrained
        pretrained_model_name_or_path: Gensyn/Qwen2.5-0.5B-Instruct
        config: trl.trainer.GRPOConfig
    num_generations: 2
    log_with: tensorboard
  data_manager:
    _target_: genrl_swarm.examples.multistage_math.data.MSMDataManager
    train_dataset: openai/gsm8k
    # num_students_to_sample, num_critics_to_sample, subsampling_method, prompt_generator_role use defaults
  run_mode: "train"